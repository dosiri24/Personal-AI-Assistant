"""
ê°œì„ ëœ ì¸í•˜ëŒ€ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬

- ê³ ì • ê³µì§€ vs ì¼ë°˜ ê³µì§€ êµ¬ë¶„
- ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ê¸°ëŠ¥
- ì²¨ë¶€íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
"""

import asyncio
import requests
import time
import json
from bs4 import BeautifulSoup, Tag
from bs4.element import NavigableString
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import re


class EnhancedInhaNoticeCrawler:
    """ê°œì„ ëœ ì¸í•˜ëŒ€ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.base_url = "https://www.inha.ac.kr/kr/950/subview.do"
        self.detail_base_url = "https://www.inha.ac.kr"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    async def crawl_notices_with_details(self, max_pages: int = 2, include_content: bool = True) -> List[Dict[str, Any]]:
        """ê³µì§€ì‚¬í•­ì„ ìƒì„¸ ë‚´ìš©ê³¼ í•¨ê»˜ í¬ë¡¤ë§"""
        notices = []
        
        for page in range(1, max_pages + 1):
            try:
                page_url = f"{self.base_url}?page={page}"
                response = self.session.get(page_url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                notice_rows = soup.select('table tbody tr')
                
                for row_idx, row in enumerate(notice_rows):
                    try:
                        cells = row.find_all('td')
                        if len(cells) < 5:
                            continue
                        
                        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                        category = cells[0].get_text(strip=True)
                        title_cell = cells[1]
                        title_link = None
                        if isinstance(title_cell, Tag):
                            title_link = title_cell.find('a')
                        
                        if title_link and isinstance(title_link, Tag):
                            title = title_link.get_text(strip=True)
                            detail_link_attr = title_link.get('href', '')
                            detail_link = str(detail_link_attr) if detail_link_attr else ""
                            if detail_link and not detail_link.startswith('http'):
                                detail_link = f"{self.detail_base_url}{detail_link}"
                        else:
                            title = title_cell.get_text(strip=True) if isinstance(title_cell, Tag) else str(title_cell)
                            detail_link = ""
                        
                        author = cells[2].get_text(strip=True)
                        date = cells[3].get_text(strip=True)
                        views = cells[4].get_text(strip=True)
                        
                        # ê³ ì • ê³µì§€ ì—¬ë¶€ íŒë‹¨
                        is_pinned = self._is_pinned_notice(row_idx, page, category, title)
                        
                        # ê¸°ë³¸ ê³µì§€ì‚¬í•­ ì •ë³´
                        notice = {
                            'category': category,
                            'title': title,
                            'author': author,
                            'date': date,
                            'views': views,
                            'link': detail_link,
                            'is_pinned': is_pinned,
                            'priority': 'high' if is_pinned else 'normal',
                            'crawled_at': datetime.now().isoformat(),
                            'page_number': page,
                            'row_index': row_idx
                        }
                        
                        # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ (ì˜µì…˜)
                        if include_content and detail_link:
                            detail_info = await self._crawl_detail_content(detail_link)
                            notice.update(detail_info)
                            
                            # ìš”ì²­ ê°„ ì§€ì—° (ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œ)
                            await asyncio.sleep(0.5)
                        
                        notices.append(notice)
                        
                        print(f"{'ğŸ“Œ' if is_pinned else 'ğŸ“„'} {title[:50]}... ìˆ˜ì§‘ ì™„ë£Œ")
                    
                    except Exception as e:
                        print(f"í–‰ íŒŒì‹± ì˜¤ë¥˜ (í˜ì´ì§€ {page}, í–‰ {row_idx}): {e}")
                        continue
                
                # í˜ì´ì§€ ê°„ ì§€ì—°
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                break
        
        # ê³ ì • ê³µì§€ë¥¼ ì•ìœ¼ë¡œ ì •ë ¬
        notices.sort(key=lambda x: (not x['is_pinned'], x['page_number'], x['row_index']))
        
        return notices
    
    def _is_pinned_notice(self, row_index: int, page: int, category: str, title: str) -> bool:
        """ê³ ì • ê³µì§€ ì—¬ë¶€ íŒë‹¨"""
        
        # ì²« í˜ì´ì§€ ìƒìœ„ ëª‡ ê°œëŠ” ê³ ì • ê³µì§€ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
        if page == 1 and row_index <= 2:
            return True
        
        # ì¹´í…Œê³ ë¦¬ë‚˜ ì œëª©ì—ì„œ ì¤‘ìš”ë„ í‚¤ì›Œë“œ í™•ì¸
        important_keywords = [
            'ì¤‘ìš”', 'í•„ìˆ˜', 'ê¸´ê¸‰', 'ì£¼ì˜', 'ê³µì§€',
            'ë“±ë¡', 'ì¥í•™', 'ëª¨ì§‘', 'ì‹ ì²­',
            'ë³€ê²½', 'ì—°ê¸°', 'ì·¨ì†Œ'
        ]
        
        high_priority_keywords = [
            'ë“±ë¡ê¸ˆ', 'ì¥í•™ê¸ˆ', 'ì¡¸ì—…', 'ì…í•™', 'ì‹œí—˜',
            'ì±„ìš©', 'ëª¨ì§‘', 'ë§ˆê°', 'ì‹ ì²­'
        ]
        
        # ì œëª©ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì²´í¬
        for keyword in high_priority_keywords:
            if keyword in title:
                return True
        
        # ì¹´í…Œê³ ë¦¬ê°€ ì¤‘ìš”í•œ ê²½ìš°
        if category in ['ì¤‘ìš”ê³µì§€', 'ê¸´ê¸‰ê³µì§€', 'í•„ìˆ˜ê³µì§€']:
            return True
        
        return False
    
    async def _crawl_detail_content(self, detail_url: str) -> Dict[str, Any]:
        """ê³µì§€ì‚¬í•­ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§"""
        detail_info = {
            'content': '',
            'attachments': [],
            'contact_info': {},
            'content_length': 0,
            'has_attachments': False
        }
        
        try:
            response = self.session.get(detail_url, timeout=10)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‚´ìš© ì¶”ì¶œ
            content = self._extract_content(soup)
            if content:
                detail_info['content'] = content
                detail_info['content_length'] = len(content)
            
            # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
            attachments = self._extract_attachments(soup)
            if attachments:
                detail_info['attachments'] = attachments
                detail_info['has_attachments'] = True
            
            # ë‹´ë‹¹ì ì •ë³´ ì¶”ì¶œ
            contact_info = self._extract_contact_info(soup)
            if contact_info:
                detail_info['contact_info'] = contact_info
        
        except Exception as e:
            print(f"ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ì˜¤ë¥˜ ({detail_url}): {e}")
            detail_info['error'] = str(e)
        
        return detail_info
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ê³µì§€ì‚¬í•­ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        content_selectors = [
            '.artclItem.viewForm',
            '.artclView',
            '.content',
            '.view_content',
            '[class*="content"]'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in content_elem(['script', 'style', 'header', 'nav', 'footer']):
                    tag.decompose()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
                content = content_elem.get_text(separator='\n', strip=True)
                
                # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
                content = re.sub(r'\n\s*\n', '\n\n', content)
                content = re.sub(r'[ \t]+', ' ', content)
                
                # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë‚´ìš©ì¸ ê²½ìš° ë°˜í™˜
                if len(content) > 100:
                    return content[:2000]  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
        
        return ""
    
    def _extract_attachments(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
        attachments = []
        
        # ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
        attachment_selectors = [
            'a[href*="download"]',
            'a[href*="file"]',
            'a[href*=".pdf"]',
            'a[href*=".doc"]',
            'a[href*=".hwp"]',
            '.file_list a',
            '.attach a'
        ]
        
        for selector in attachment_selectors:
            for link in soup.select(selector):
                if isinstance(link, Tag):
                    filename = link.get_text(strip=True)
                    file_url_attr = link.get('href', '')
                    file_url = str(file_url_attr) if file_url_attr else ""
                    
                    # íŒŒì¼ í™•ì¥ì í™•ì¸
                    if any(ext in filename.lower() for ext in ['.pdf', '.doc', '.hwp', '.xlsx', '.zip']):
                        if file_url and not file_url.startswith('http'):
                            file_url = f"{self.detail_base_url}{file_url}"
                        
                        attachments.append({
                            'name': filename,
                            'url': file_url,
                            'type': self._get_file_type(filename)
                        })
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_attachments = []
        for attach in attachments:
            if attach['name'] not in seen:
                seen.add(attach['name'])
                unique_attachments.append(attach)
        
        return unique_attachments
    
    def _extract_contact_info(self, soup: BeautifulSoup) -> Dict[str, str]:
        """ë‹´ë‹¹ì ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ"""
        contact_info = {}
        
        # í…ìŠ¤íŠ¸ì—ì„œ ì—°ë½ì²˜ ì •ë³´ íŒ¨í„´ ë§¤ì¹­
        text = soup.get_text()
        
        # ë‹´ë‹¹ì ì´ë¦„
        name_pattern = r'ë‹´ë‹¹ì[:\s]*([ê°€-í£]+)'
        name_match = re.search(name_pattern, text)
        if name_match:
            contact_info['contact_person'] = name_match.group(1)
        
        # ì „í™”ë²ˆí˜¸
        phone_pattern = r'ì—°ë½ì²˜[:\s]*([\d-]+)'
        phone_match = re.search(phone_pattern, text)
        if phone_match:
            contact_info['phone'] = phone_match.group(1)
        
        # ì´ë©”ì¼
        email_pattern = r'ì´ë©”ì¼[:\s]*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
        email_match = re.search(email_pattern, text)
        if email_match:
            contact_info['email'] = email_match.group(1)
        
        return contact_info
    
    def _get_file_type(self, filename: str) -> str:
        """íŒŒì¼ íƒ€ì… ë¶„ë¥˜"""
        filename_lower = filename.lower()
        
        if '.pdf' in filename_lower:
            return 'PDF'
        elif any(ext in filename_lower for ext in ['.doc', '.docx']):
            return 'Word'
        elif '.hwp' in filename_lower:
            return 'HWP'
        elif any(ext in filename_lower for ext in ['.xls', '.xlsx']):
            return 'Excel'
        elif any(ext in filename_lower for ext in ['.zip', '.rar']):
            return 'Archive'
        else:
            return 'Other'
    
    def save_enhanced_results(self, notices: List[Dict[str, Any]], filename: str = "inha_notices_enhanced.json"):
        """ê°œì„ ëœ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        # í†µê³„ ì •ë³´ ìƒì„±
        total_notices = len(notices)
        pinned_notices = len([n for n in notices if n['is_pinned']])
        with_content = len([n for n in notices if n.get('content')])
        with_attachments = len([n for n in notices if n.get('has_attachments')])
        
        result = {
            'crawl_info': {
                'timestamp': datetime.now().isoformat(),
                'total_notices': total_notices,
                'pinned_notices': pinned_notices,
                'notices_with_content': with_content,
                'notices_with_attachments': with_attachments
            },
            'notices': notices
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ í†µê³„:")
        print(f"- ì´ ê³µì§€ì‚¬í•­: {total_notices}ê°œ")
        print(f"- ê³ ì • ê³µì§€: {pinned_notices}ê°œ")
        print(f"- ìƒì„¸ ë‚´ìš© í¬í•¨: {with_content}ê°œ")
        print(f"- ì²¨ë¶€íŒŒì¼ í¬í•¨: {with_attachments}ê°œ")
        print(f"- ì €ì¥ íŒŒì¼: {filename}")
    
    def get_summary(self, notices: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½"""
        
        # ê³ ì • ê³µì§€ì™€ ì¼ë°˜ ê³µì§€ ë¶„ë¦¬
        pinned = [n for n in notices if n['is_pinned']]
        regular = [n for n in notices if not n['is_pinned']]
        
        return {
            'total_count': len(notices),
            'pinned_count': len(pinned),
            'regular_count': len(regular),
            'pinned_notices': [
                {
                    'title': n['title'],
                    'date': n['date'],
                    'category': n['category'],
                    'has_content': bool(n.get('content')),
                    'attachments_count': len(n.get('attachments', []))
                }
                for n in pinned[:5]  # ìƒìœ„ 5ê°œë§Œ
            ],
            'latest_regular': [
                {
                    'title': n['title'],
                    'date': n['date'],
                    'category': n['category'],
                    'has_content': bool(n.get('content')),
                    'attachments_count': len(n.get('attachments', []))
                }
                for n in regular[:10]  # ìƒìœ„ 10ê°œë§Œ
            ]
        }


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ê°œì„ ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    crawler = EnhancedInhaNoticeCrawler()
    
    print("ğŸš€ ê°œì„ ëœ ì¸í•˜ëŒ€ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘...")
    print("- ê³ ì • ê³µì§€ vs ì¼ë°˜ ê³µì§€ êµ¬ë¶„")
    print("- ìƒì„¸ ë‚´ìš© ë° ì²¨ë¶€íŒŒì¼ ì •ë³´ ìˆ˜ì§‘")
    print()
    
    # ìƒì„¸ ë‚´ìš©ê³¼ í•¨ê»˜ í¬ë¡¤ë§ (ì²˜ìŒ ëª‡ ê°œë§Œ)
    notices = await crawler.crawl_notices_with_details(max_pages=1, include_content=True)
    
    # ê²°ê³¼ ì €ì¥
    crawler.save_enhanced_results(notices)
    
    # ìš”ì•½ ì •ë³´ ì¶œë ¥
    summary = crawler.get_summary(notices)
    
    print(f"\nğŸ“Œ ê³ ì • ê³µì§€ ({summary['pinned_count']}ê°œ):")
    for notice in summary['pinned_notices']:
        attachments_info = f" ğŸ“{notice['attachments_count']}ê°œ" if notice['attachments_count'] > 0 else ""
        content_info = " ğŸ“„ìƒì„¸" if notice['has_content'] else ""
        print(f"- {notice['title'][:50]}... ({notice['date']}){attachments_info}{content_info}")
    
    print(f"\nğŸ“„ ìµœì‹  ì¼ë°˜ ê³µì§€ ({min(10, summary['regular_count'])}ê°œ):")
    for notice in summary['latest_regular'][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        attachments_info = f" ğŸ“{notice['attachments_count']}ê°œ" if notice['attachments_count'] > 0 else ""
        content_info = " ğŸ“„ìƒì„¸" if notice['has_content'] else ""
        print(f"- {notice['title'][:50]}... ({notice['date']}){attachments_info}{content_info}")


if __name__ == "__main__":
    asyncio.run(main())
