"""
ì›¹ ì •ë³´ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ

ì •ê¸°ì ì¸ ì›¹ í¬ë¡¤ë§ ì‘ì—…ì„ ê´€ë¦¬í•˜ê³ 
ë³€ê²½ ê°ì§€ ë° ì•Œë¦¼ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import json
import hashlib
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import time
import sys
import os

# í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ë¥¼ ì§ì ‘ import
sys.path.append(os.path.dirname(os.path.abspath(__file__)))


@dataclass
class CrawlJob:
    """í¬ë¡¤ë§ ì‘ì—… ì •ì˜"""
    job_id: str
    name: str
    url: str
    schedule_pattern: str  # cron ìŠ¤íƒ€ì¼: "*/30 * * * *" (ë§¤ 30ë¶„)
    max_pages: int = 3
    enabled: bool = True
    last_run: Optional[str] = None
    last_hash: Optional[str] = None
    change_detected: bool = False
    error_count: int = 0
    max_errors: int = 5


@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼"""
    job_id: str
    timestamp: str
    success: bool
    data_count: int
    changes_detected: bool
    content_hash: str
    error_message: Optional[str] = None
    execution_time: float = 0.0


class WebCrawlScheduler:
    """ì›¹ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, data_dir: str = "data/crawl_results"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.jobs: Dict[str, CrawlJob] = {}
        self.results: List[CrawlResult] = []
        self.logger = logging.getLogger(__name__)
        self.is_running = False
        
        # ë³€ê²½ ê°ì§€ ì½œë°±
        self.change_callbacks: List[Callable[[CrawlResult], None]] = []
        
        # ê¸°ë³¸ ì¸í•˜ëŒ€ í¬ë¡¤ë§ ì‘ì—… ë“±ë¡
        self._register_default_jobs()
    
    def _register_default_jobs(self):
        """ê¸°ë³¸ í¬ë¡¤ë§ ì‘ì—… ë“±ë¡"""
        inha_job = CrawlJob(
            job_id="inha_notices",
            name="ì¸í•˜ëŒ€ ê³µì§€ì‚¬í•­",
            url="https://www.inha.ac.kr/kr/950/subview.do",
            schedule_pattern="*/30 * * * *",  # 30ë¶„ë§ˆë‹¤
            max_pages=2
        )
        self.add_job(inha_job)
    
    def add_job(self, job: CrawlJob):
        """í¬ë¡¤ë§ ì‘ì—… ì¶”ê°€"""
        self.jobs[job.job_id] = job
        self.logger.info(f"í¬ë¡¤ë§ ì‘ì—… ì¶”ê°€ë¨: {job.name} ({job.schedule_pattern})")
    
    def remove_job(self, job_id: str):
        """í¬ë¡¤ë§ ì‘ì—… ì œê±°"""
        if job_id in self.jobs:
            del self.jobs[job_id]
            self.logger.info(f"í¬ë¡¤ë§ ì‘ì—… ì œê±°ë¨: {job_id}")
    
    def enable_job(self, job_id: str):
        """ì‘ì—… í™œì„±í™”"""
        if job_id in self.jobs:
            self.jobs[job_id].enabled = True
            self.logger.info(f"ì‘ì—… í™œì„±í™”ë¨: {job_id}")
    
    def disable_job(self, job_id: str):
        """ì‘ì—… ë¹„í™œì„±í™”"""
        if job_id in self.jobs:
            self.jobs[job_id].enabled = False
            self.logger.info(f"ì‘ì—… ë¹„í™œì„±í™”ë¨: {job_id}")
    
    async def execute_job(self, job: CrawlJob) -> CrawlResult:
        """ë‹¨ì¼ í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰"""
        start_time = time.time()
        timestamp = datetime.now().isoformat()
        
        try:
            self.logger.info(f"í¬ë¡¤ë§ ì‘ì—… ì‹œì‘: {job.name}")
            
            # ì¸í•˜ëŒ€ í¬ë¡¤ëŸ¬ ì‹¤í–‰
            if job.job_id == "inha_notices":
                notices = await self._crawl_inha_notices(job.max_pages)
                
                # ë°ì´í„° í•´ì‹œ ê³„ì‚°
                content_hash = self._calculate_hash(notices)
                
                # ë³€ê²½ ê°ì§€
                changes_detected = (job.last_hash is None or 
                                  content_hash != job.last_hash)
                
                # ê²°ê³¼ ì €ì¥
                if notices:
                    result_file = self.data_dir / f"{job.job_id}_{timestamp[:10]}.json"
                    with open(result_file, 'w', encoding='utf-8') as f:
                        json.dump(notices, f, ensure_ascii=False, indent=2)
                
                # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
                job.last_run = timestamp
                job.last_hash = content_hash
                job.change_detected = changes_detected
                job.error_count = 0
                
                execution_time = time.time() - start_time
                
                result = CrawlResult(
                    job_id=job.job_id,
                    timestamp=timestamp,
                    success=True,
                    data_count=len(notices),
                    changes_detected=changes_detected,
                    content_hash=content_hash,
                    execution_time=execution_time
                )
                
                self.logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {job.name} - {len(notices)}ê°œ í•­ëª©, ë³€ê²½: {changes_detected}")
                
                # ë³€ê²½ ê°ì§€ ì‹œ ì½œë°± ì‹¤í–‰
                if changes_detected and self.change_callbacks:
                    for callback in self.change_callbacks:
                        try:
                            callback(result)
                        except Exception as e:
                            self.logger.error(f"ë³€ê²½ ê°ì§€ ì½œë°± ì˜¤ë¥˜: {e}")
                
                return result
            
            else:
                raise NotImplementedError(f"í¬ë¡¤ëŸ¬ê°€ êµ¬í˜„ë˜ì§€ ì•Šì€ ì‘ì—…: {job.job_id}")
        
        except Exception as e:
            execution_time = time.time() - start_time
            job.error_count += 1
            
            error_result = CrawlResult(
                job_id=job.job_id,
                timestamp=timestamp,
                success=False,
                data_count=0,
                changes_detected=False,
                content_hash="",
                error_message=str(e),
                execution_time=execution_time
            )
            
            self.logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {job.name} - {e}")
            return error_result
    
    def _calculate_hash(self, data: Any) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚°"""
        json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(json_str.encode()).hexdigest()
    
    async def _crawl_inha_notices(self, max_pages: int = 2) -> List[Dict[str, Any]]:
        """ê°œì„ ëœ ì¸í•˜ëŒ€ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ (ê³ ì •ê³µì§€ êµ¬ë¶„, ìƒì„¸ë‚´ìš© í¬í•¨)"""
        import requests
        from bs4 import BeautifulSoup
        import re
        
        notices = []
        base_url = "https://www.inha.ac.kr/kr/950/subview.do"
        detail_base_url = "https://www.inha.ac.kr"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        for page in range(1, max_pages + 1):
            try:
                page_url = f"{base_url}?page={page}"
                response = requests.get(page_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                notice_rows = soup.select('table tbody tr')
                
                for row_idx, row in enumerate(notice_rows):
                    try:
                        cells = row.find_all('td')
                        if len(cells) < 5:
                            continue
                        
                        category = cells[0].get_text(strip=True)
                        title_cell = cells[1]
                        title_link = title_cell.find('a')
                        
                        if title_link:
                            title = title_link.get_text(strip=True)
                            detail_link = title_link.get('href', '')
                            if detail_link and not detail_link.startswith('http'):
                                detail_link = f"{detail_base_url}{detail_link}"
                        else:
                            title = title_cell.get_text(strip=True)
                            detail_link = ""
                        
                        author = cells[2].get_text(strip=True)
                        date = cells[3].get_text(strip=True)
                        views = cells[4].get_text(strip=True)
                        
                        # ê³ ì • ê³µì§€ ì—¬ë¶€ íŒë‹¨
                        is_pinned = self._is_pinned_notice(row_idx, page, category, title)
                        
                        notice = {
                            'category': category,
                            'title': title,
                            'author': author,
                            'date': date,
                            'views': views,
                            'link': detail_link,
                            'is_pinned': is_pinned,
                            'priority': 'high' if is_pinned else 'normal',
                            'crawled_at': datetime.now().isoformat(),
                            'page_number': page,
                            'row_index': row_idx
                        }
                        
                        # ì¤‘ìš” ê³µì§€ë‚˜ ìµœì‹  ê³µì§€ë§Œ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ (ì„±ëŠ¥ ê³ ë ¤)
                        if is_pinned and detail_link:
                            try:
                                detail_info = await self._crawl_detail_content(detail_link)
                                notice.update(detail_info)
                            except Exception as e:
                                self.logger.error(f"ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        
                        notices.append(notice)
                    
                    except Exception as e:
                        self.logger.error(f"í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                # í˜ì´ì§€ ê°„ ì§€ì—°
                await asyncio.sleep(1)
                
            except Exception as e:
                self.logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                break
        
        # ê³ ì • ê³µì§€ë¥¼ ì•ìœ¼ë¡œ ì •ë ¬
        notices.sort(key=lambda x: (not x['is_pinned'], x['page_number'], x['row_index']))
        
        return notices
    
    def _is_pinned_notice(self, row_index: int, page: int, category: str, title: str) -> bool:
        """ê³ ì • ê³µì§€ ì—¬ë¶€ íŒë‹¨"""
        
        # ì²« í˜ì´ì§€ ìƒìœ„ ëª‡ ê°œëŠ” ê³ ì • ê³µì§€ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
        if page == 1 and row_index <= 2:
            return True
        
        # ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
        high_priority_keywords = [
            'ë“±ë¡ê¸ˆ', 'ì¥í•™ê¸ˆ', 'ì¡¸ì—…', 'ì…í•™', 'ì‹œí—˜',
            'ì±„ìš©', 'ëª¨ì§‘', 'ë§ˆê°', 'ì‹ ì²­', 'íŠœí„°ë§',
            'ë©˜í† ë§', 'í”„ë¡œê·¸ë¨'
        ]
        
        # ì œëª©ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì²´í¬
        for keyword in high_priority_keywords:
            if keyword in title:
                return True
        
        return False
    
    async def _crawl_detail_content(self, detail_url: str) -> Dict[str, Any]:
        """ê³µì§€ì‚¬í•­ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§"""
        detail_info = {
            'content': '',
            'attachments': [],
            'contact_info': {},
            'content_length': 0
        }
        
        try:
            import requests
            from bs4 import BeautifulSoup
            import re
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(detail_url, headers=headers, timeout=10)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‚´ìš© ì¶”ì¶œ
            content_elem = soup.select_one('.artclItem.viewForm')
            if content_elem:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in content_elem(['script', 'style']):
                    tag.decompose()
                
                content = content_elem.get_text(separator='\n', strip=True)
                content = re.sub(r'\n\s*\n', '\n\n', content)
                content = re.sub(r'[ \t]+', ' ', content)
                
                if len(content) > 50:
                    detail_info['content'] = content[:1000]  # ìµœëŒ€ 1000ìë¡œ ì œí•œ
                    detail_info['content_length'] = len(content)
            
            # ë‹´ë‹¹ì ì •ë³´ ì¶”ì¶œ
            text = soup.get_text()
            
            # ë‹´ë‹¹ì ì´ë¦„
            name_match = re.search(r'ë‹´ë‹¹ì[:\s]*([ê°€-í£]+)', text)
            if name_match:
                detail_info['contact_info']['contact_person'] = name_match.group(1)
            
            # ì „í™”ë²ˆí˜¸
            phone_match = re.search(r'ì—°ë½ì²˜[:\s]*([\d-]+)', text)
            if phone_match:
                detail_info['contact_info']['phone'] = phone_match.group(1)
        
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return detail_info
    
    async def run_once(self) -> List[CrawlResult]:
        """ëª¨ë“  í™œì„± ì‘ì—…ì„ í•œ ë²ˆ ì‹¤í–‰"""
        results = []
        
        for job in self.jobs.values():
            if job.enabled and job.error_count < job.max_errors:
                result = await self.execute_job(job)
                results.append(result)
                self.results.append(result)
        
        return results
    
    async def run_scheduler(self, interval: int = 60):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ë©”ì¸ ë£¨í”„ (ì´ˆ ë‹¨ìœ„ ê°„ê²©)"""
        self.is_running = True
        self.logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨ (ê°„ê²©: {interval}ì´ˆ)")
        
        while self.is_running:
            try:
                current_time = datetime.now()
                
                for job in self.jobs.values():
                    if (job.enabled and 
                        job.error_count < job.max_errors and
                        self._should_run_job(job, current_time)):
                        
                        result = await self.execute_job(job)
                        self.results.append(result)
                
                await asyncio.sleep(interval)
                
            except Exception as e:
                self.logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(interval)
    
    def _should_run_job(self, job: CrawlJob, current_time: datetime) -> bool:
        """ì‘ì—… ì‹¤í–‰ ì‹œì  íŒë‹¨"""
        if job.last_run is None:
            return True
        
        try:
            last_run_time = datetime.fromisoformat(job.last_run)
            
            # ê°„ë‹¨í•œ ê°„ê²© ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§
            if "*/30" in job.schedule_pattern:  # 30ë¶„ë§ˆë‹¤
                return current_time - last_run_time >= timedelta(minutes=30)
            elif "*/60" in job.schedule_pattern:  # 1ì‹œê°„ë§ˆë‹¤
                return current_time - last_run_time >= timedelta(hours=1)
            elif "0 */6" in job.schedule_pattern:  # 6ì‹œê°„ë§ˆë‹¤
                return current_time - last_run_time >= timedelta(hours=6)
            elif "0 0" in job.schedule_pattern:  # ë§¤ì¼
                return current_time - last_run_time >= timedelta(days=1)
            
            # ê¸°ë³¸: 30ë¶„ ê°„ê²©
            return current_time - last_run_time >= timedelta(minutes=30)
            
        except Exception:
            return True
    
    def stop_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        self.is_running = False
        self.logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ë¨")
    
    def add_change_callback(self, callback: Callable[[CrawlResult], None]):
        """ë³€ê²½ ê°ì§€ ì½œë°± ì¶”ê°€"""
        self.change_callbacks.append(callback)
        self.logger.info("ë³€ê²½ ê°ì§€ ì½œë°± ì¶”ê°€ë¨")
    
    def get_status(self) -> Dict[str, Any]:
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
        active_jobs = sum(1 for job in self.jobs.values() if job.enabled)
        recent_results = [r for r in self.results if 
                         datetime.fromisoformat(r.timestamp) > 
                         datetime.now() - timedelta(hours=24)]
        
        return {
            "is_running": self.is_running,
            "total_jobs": len(self.jobs),
            "active_jobs": active_jobs,
            "recent_results": len(recent_results),
            "jobs": [asdict(job) for job in self.jobs.values()],
            "last_24h_results": [asdict(r) for r in recent_results[-10:]]
        }
    
    def get_latest_changes(self, hours: int = 24) -> List[CrawlResult]:
        """ìµœê·¼ ë³€ê²½ ì‚¬í•­ ì¡°íšŒ"""
        cutoff = datetime.now() - timedelta(hours=hours)
        
        return [r for r in self.results if 
                r.changes_detected and 
                datetime.fromisoformat(r.timestamp) > cutoff]
    
    def save_state(self, filename: str = "scheduler_state.json"):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì €ì¥"""
        state = {
            "jobs": {job_id: asdict(job) for job_id, job in self.jobs.items()},
            "results": [asdict(r) for r in self.results[-100:]]  # ìµœê·¼ 100ê°œë§Œ
        }
        
        state_file = self.data_dir / filename
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì €ì¥ë¨: {state_file}")
    
    def load_state(self, filename: str = "scheduler_state.json"):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë³µì›"""
        state_file = self.data_dir / filename
        
        if state_file.exists():
            try:
                with open(state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                
                # ì‘ì—… ë³µì›
                for job_id, job_data in state.get("jobs", {}).items():
                    self.jobs[job_id] = CrawlJob(**job_data)
                
                # ê²°ê³¼ ë³µì›
                for result_data in state.get("results", []):
                    self.results.append(CrawlResult(**result_data))
                
                self.logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë³µì›ë¨: {len(self.jobs)}ê°œ ì‘ì—…")
                
            except Exception as e:
                self.logger.error(f"ìƒíƒœ ë³µì› ì‹¤íŒ¨: {e}")


# ë³€ê²½ ê°ì§€ ì½œë°± ì˜ˆì‹œ
def on_content_change(result: CrawlResult):
    """ì½˜í…ì¸  ë³€ê²½ ì‹œ ì•Œë¦¼"""
    print(f"ğŸ”” ë³€ê²½ ê°ì§€: {result.job_id}")
    print(f"   ì‹œê°„: {result.timestamp}")
    print(f"   í•­ëª© ìˆ˜: {result.data_count}")
    print(f"   í•´ì‹œ: {result.content_hash[:8]}...")


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸"""
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
    scheduler = WebCrawlScheduler()
    
    # ë³€ê²½ ê°ì§€ ì½œë°± ë“±ë¡
    scheduler.add_change_callback(on_content_change)
    
    # ìƒíƒœ ë³µì›
    scheduler.load_state()
    
    print("ğŸ“… ì›¹ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
    print(f"ë“±ë¡ëœ ì‘ì—…: {len(scheduler.jobs)}ê°œ")
    
    # í•œ ë²ˆ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ ëª¨ë“  ì‘ì—… ì‹¤í–‰ ì¤‘...")
    results = await scheduler.run_once()
    
    for result in results:
        status = "âœ… ì„±ê³µ" if result.success else "âŒ ì‹¤íŒ¨"
        change = "ğŸ”„ ë³€ê²½ë¨" if result.changes_detected else "âšª ë³€ê²½ì—†ìŒ"
        print(f"{status} {change} {result.job_id}: {result.data_count}ê°œ í•­ëª©")
    
    # ìƒíƒœ í™•ì¸
    status = scheduler.get_status()
    print(f"\nğŸ“Š ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ:")
    print(f"- í™œì„± ì‘ì—…: {status['active_jobs']}/{status['total_jobs']}")
    print(f"- ìµœê·¼ 24ì‹œê°„ ê²°ê³¼: {status['recent_results']}ê°œ")
    
    # ìµœê·¼ ë³€ê²½ì‚¬í•­
    changes = scheduler.get_latest_changes()
    if changes:
        print(f"\nğŸ”” ìµœê·¼ ë³€ê²½ì‚¬í•­ ({len(changes)}ê°œ):")
        for change in changes[-3:]:
            print(f"- {change.job_id}: {change.timestamp[:19]}")
    
    # ìƒíƒœ ì €ì¥
    scheduler.save_state()
    
    # ì§€ì†ì  ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ì£¼ì„ ì²˜ë¦¬)
    # print("\nâ° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (Ctrl+Cë¡œ ì¤‘ì§€)")
    # try:
    #     await scheduler.run_scheduler(interval=30)
    # except KeyboardInterrupt:
    #     print("\nâ¹ï¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ë¨")
    #     scheduler.stop_scheduler()


if __name__ == "__main__":
    asyncio.run(main())
