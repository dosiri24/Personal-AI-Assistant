"""
ê¸°ì–µ ê´€ë¦¬ ì‹œìŠ¤í…œ (Memory Manager) - ë²¡í„° ìŠ¤í† ì–´ í†µí•© ë²„ì „

ì´ ëª¨ë“ˆì€ AI ì‹œìŠ¤í…œì˜ ì¥ê¸°ê¸°ì–µ ìƒëª…ì£¼ê¸°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥:
- VectorStoreì™€ ì™„ì „ í†µí•©
- ìë™ ì¤‘ìš”ë„ íŒë‹¨ ë° ì—…ë°ì´íŠ¸
- ê¸°ì–µ ì••ì¶• ë° ìš”ì•½
- ì˜¤ë˜ëœ ê¸°ì–µ ì•„ì¹´ì´ë¹™
- ê¸°ì–µ ì •ë¦¬ ë° ìµœì í™”

Author: Personal AI Assistant
Created: 2025-01-03
"""

from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import math
import statistics
from collections import defaultdict, Counter
import copy

from .enhanced_models import (
    BaseMemory, MetadataSchema, ImportanceLevel,
    MemoryType, MemoryStatus
)
from .vector_store import VectorStore, VectorDocument, CollectionType
from .rag_engine import RAGSearchEngine, SearchQuery, SearchResult
from ..ai_engine.llm_provider import LLMProvider, ChatMessage, LLMResponse
from ..utils.logger import get_logger

logger = get_logger(__name__)


class ArchiveReason(Enum):
    """ì•„ì¹´ì´ë¹™ ì´ìœ """
    LOW_IMPORTANCE = "low_importance"
    OLD_AGE = "old_age"
    REDUNDANT = "redundant"
    USER_REQUEST = "user_request"
    STORAGE_LIMIT = "storage_limit"


class CompressionStrategy(Enum):
    """ì••ì¶• ì „ëµ"""
    SUMMARIZE = "summarize"
    EXTRACT_KEY_POINTS = "extract_key_points"
    MERGE_SIMILAR = "merge_similar"
    CLUSTER_AND_REPRESENT = "cluster_and_represent"


@dataclass
class ArchiveEntry:
    """ì•„ì¹´ì´ë¸Œ ì—”íŠ¸ë¦¬"""
    memory_id: str
    original_memory: BaseMemory
    archive_date: datetime
    archive_reason: ArchiveReason
    compressed_content: Optional[str] = None
    compression_ratio: float = 1.0


@dataclass
class MemoryStatistics:
    """ê¸°ì–µ í†µê³„"""
    total_memories: int = 0
    active_memories: int = 0
    archived_memories: int = 0
    average_importance: float = 0.0
    memory_by_type: Dict[MemoryType, int] = field(default_factory=dict)
    memory_by_importance: Dict[ImportanceLevel, int] = field(default_factory=dict)
    storage_usage_mb: float = 0.0
    oldest_memory_age_days: int = 0
    newest_memory_age_days: int = 0


class MemoryManager:
    """
    ê¸°ì–µ ê´€ë¦¬ ì‹œìŠ¤í…œ - VectorStore í†µí•© ë²„ì „
    
    ê¸°ì–µì˜ ì „ì²´ ìƒëª…ì£¼ê¸°ë¥¼ ê´€ë¦¬í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        vector_store: VectorStore,
        rag_engine: Optional[RAGSearchEngine] = None,
        llm_provider: Optional[LLMProvider] = None,
        max_active_memories: int = 10000,
        archive_threshold_days: int = 365,
        low_importance_threshold: float = 0.3,
        compression_threshold_days: int = 30
    ):
        self.vector_store = vector_store
        self.rag_engine = rag_engine
        self.llm_provider = llm_provider
        
        # ì„¤ì •
        self.max_active_memories = max_active_memories
        self.archive_threshold_days = archive_threshold_days
        self.low_importance_threshold = low_importance_threshold
        self.compression_threshold_days = compression_threshold_days
        
        # ì•„ì¹´ì´ë¸Œ ì €ì¥ì†Œ
        self.archive: Dict[str, ArchiveEntry] = {}
        
        # í†µê³„ ìºì‹œ
        self._last_statistics_update = datetime.now()
        self._cached_statistics: Optional[MemoryStatistics] = None
        
        # ë©”ëª¨ë¦¬ íƒ€ì…ë³„ ì»¬ë ‰ì…˜ ë§¤í•‘
        self.type_to_collection = {
            MemoryType.ACTION: CollectionType.ACTION_MEMORY,
            MemoryType.CONVERSATION: CollectionType.CONVERSATION,
            MemoryType.PROJECT: CollectionType.PROJECT_CONTEXT,
            MemoryType.PREFERENCE: CollectionType.USER_PREFERENCE,
            MemoryType.SYSTEM: CollectionType.SYSTEM_STATE,
            MemoryType.LEARNING: CollectionType.ACTION_MEMORY,
            MemoryType.CONTEXT: CollectionType.PROJECT_CONTEXT,
            MemoryType.RELATIONSHIP: CollectionType.USER_PREFERENCE
        }
    
    async def initialize(self) -> None:
        """ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        logger.info("ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹œì‘")
        
        try:
            # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
            await self.vector_store.initialize()
            
            # ì´ˆê¸° í†µê³„ ê³„ì‚°
            await self.update_statistics()
            
            logger.info("ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def add_memory(self, memory: BaseMemory) -> str:
        """
        ìƒˆë¡œìš´ ê¸°ì–µ ì¶”ê°€
        
        Args:
            memory: ì¶”ê°€í•  ê¸°ì–µ
            
        Returns:
            memory_id: ìƒì„±ëœ ê¸°ì–µ ID
        """
        try:
            # ìë™ ì¤‘ìš”ë„ ê³„ì‚°
            if memory.metadata.importance_score == 0.0:
                memory.metadata.importance_score = await self._calculate_importance(memory)
                memory.metadata.importance_level = self._score_to_level(
                    memory.metadata.importance_score
                )
            
            # VectorDocumentë¡œ ë³€í™˜
            vector_doc = self._memory_to_vector_doc(memory)
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            collection_type = self.type_to_collection.get(
                memory.memory_type, 
                CollectionType.ACTION_MEMORY
            )
            success = await self.vector_store.add_document(collection_type, vector_doc)
            
            if not success:
                raise RuntimeError(f"ê¸°ì–µ ì¶”ê°€ ì‹¤íŒ¨: {memory.id}")
            
            # RAG ì—”ì§„ì— ì¸ë±ì‹± (ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”)
            # if self.rag_engine:
            #     try:
            #         await self.rag_engine.index_memory(memory)
            #     except Exception as e:
            #         logger.warning(f"RAG ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
            #         # RAG ì¸ë±ì‹± ì‹¤íŒ¨í•´ë„ ê¸°ì–µ ì¶”ê°€ëŠ” ê³„ì† ì§„í–‰
            
            # ìŠ¤í† ë¦¬ì§€ ìš©ëŸ‰ ì²´í¬
            await self._check_storage_limits()
            
            logger.info(f"ìƒˆë¡œìš´ ê¸°ì–µ ì¶”ê°€: {memory.id} (ì¤‘ìš”ë„: {memory.metadata.importance_score:.3f})")
            return memory.id
            
        except Exception as e:
            logger.error(f"ê¸°ì–µ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def _memory_to_vector_doc(self, memory: BaseMemory) -> VectorDocument:
        """BaseMemoryë¥¼ VectorDocumentë¡œ ë³€í™˜"""
        metadata = {
            'user_id': memory.user_id,
            'memory_type': memory.memory_type.value,
            'importance': memory.importance.value,
            'importance_score': memory.metadata.importance_score,
            'created_at': memory.created_at.isoformat(),
            'updated_at': memory.updated_at.isoformat(),
            'last_accessed': memory.last_accessed.isoformat(),
            'accessed_count': memory.accessed_count,
            'tags': ','.join(memory.tags),  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            'keywords': ','.join(memory.keywords),  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            'status': memory.status.value,
            'is_archived': memory.is_archived,
            'source': memory.source
        }
        
        return VectorDocument(
            id=memory.id,
            content=memory.content,
            metadata=metadata,
            timestamp=memory.created_at
        )
    
    def _vector_doc_to_memory(self, doc: Dict[str, Any]) -> BaseMemory:
        """VectorDocument ê²°ê³¼ë¥¼ BaseMemoryë¡œ ë³€í™˜"""
        # ChromaDB ê²°ê³¼ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
        metadata_dict = doc.get('metadatas', [{}])[0] if doc.get('metadatas') else {}
        
        # ë¬¸ìì—´ë¡œ ì €ì¥ëœ tagsì™€ keywordsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³µì›
        tags_str = metadata_dict.get('tags', '')
        tags = [tag.strip() for tag in tags_str.split(',') if tag.strip()] if tags_str else []
        
        keywords_str = metadata_dict.get('keywords', '')
        keywords = [kw.strip() for kw in keywords_str.split(',') if kw.strip()] if keywords_str else []
        
        # ì¤‘ìš”ë„ ë ˆë²¨ ë³€í™˜ - ë¬¸ìì—´ì—ì„œ ImportanceLevelë¡œ
        importance_str = metadata_dict.get('importance', 'medium')
        importance_map = {
            'critical': ImportanceLevel.CRITICAL,
            'high': ImportanceLevel.HIGH,
            'medium': ImportanceLevel.MEDIUM,
            'low': ImportanceLevel.LOW,
            'minimal': ImportanceLevel.MINIMAL,
            'trivial': ImportanceLevel.TRIVIAL
        }
        importance = importance_map.get(importance_str.lower(), ImportanceLevel.MEDIUM)
        
        memory = BaseMemory(
            id=doc.get('ids', [''])[0],
            user_id=metadata_dict.get('user_id', ''),
            memory_type=MemoryType(metadata_dict.get('memory_type', 'action')),
            content=doc.get('documents', [''])[0],
            importance=importance,
            source=metadata_dict.get('source', 'system'),
            created_at=datetime.fromisoformat(
                metadata_dict.get('created_at', datetime.now().isoformat())
            ),
            updated_at=datetime.fromisoformat(
                metadata_dict.get('updated_at', datetime.now().isoformat())
            ),
            last_accessed=datetime.fromisoformat(
                metadata_dict.get('last_accessed', datetime.now().isoformat())
            ),
            accessed_count=metadata_dict.get('accessed_count', 0),
            tags=tags,
            keywords=keywords,
            status=MemoryStatus(metadata_dict.get('status', 'active')),
            is_archived=metadata_dict.get('is_archived', False)
        )
        
        # ë©”íƒ€ë°ì´í„° ë³µì›
        memory.metadata.importance_score = metadata_dict.get('importance_score', 0.0)
        memory.metadata.importance_level = importance
        
        return memory
    
    async def get_memory(self, memory_id: str) -> Optional[BaseMemory]:
        """ê¸°ì–µ ì¡°íšŒ"""
        # ëª¨ë“  ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
        for collection_type in CollectionType:
            try:
                doc = await self.vector_store.get_document(collection_type, memory_id)
                if doc:
                    return self._vector_doc_to_memory(doc)
            except Exception as e:
                logger.debug(f"ì»¬ë ‰ì…˜ {collection_type.value}ì—ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
        
        return None
    
    async def update_memory_importance(self, memory_id: str) -> float:
        """
        ê¸°ì–µì˜ ì¤‘ìš”ë„ ì—…ë°ì´íŠ¸
        
        Args:
            memory_id: ê¸°ì–µ ID
            
        Returns:
            ìƒˆë¡œìš´ ì¤‘ìš”ë„ ì ìˆ˜
        """
        memory = await self.get_memory(memory_id)
        if not memory:
            logger.warning(f"ê¸°ì–µì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {memory_id}")
            return 0.0
        
        # ì¤‘ìš”ë„ ì¬ê³„ì‚°
        new_score = await self._calculate_importance(memory)
        memory.metadata.importance_score = new_score
        memory.metadata.importance_level = self._score_to_level(new_score)
        memory.updated_at = datetime.now()
        
        # TODO: ì‹¤ì œ ì—…ë°ì´íŠ¸ëŠ” VectorStoreì˜ update ë©”ì„œë“œ êµ¬í˜„ í•„ìš”
        # í˜„ì¬ëŠ” ì¤‘ìš”ë„ ê³„ì‚°ë§Œ ìˆ˜í–‰
        
        logger.info(f"ê¸°ì–µ ì¤‘ìš”ë„ ê³„ì‚°: {memory_id} -> {new_score:.3f}")
        return new_score
    
    async def compress_old_memories(self, 
                                  strategy: CompressionStrategy = CompressionStrategy.SUMMARIZE) -> int:
        """
        ì˜¤ë˜ëœ ê¸°ì–µë“¤ì„ ì••ì¶•
        
        Args:
            strategy: ì••ì¶• ì „ëµ
            
        Returns:
            ì••ì¶•ëœ ê¸°ì–µì˜ ìˆ˜
        """
        if not self.llm_provider:
            logger.warning("LLM í”„ë¡œë°”ì´ë”ê°€ ì—†ì–´ì„œ ì••ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return 0
        
        compressed_count = 0
        
        # ëª¨ë“  ì»¬ë ‰ì…˜ì—ì„œ ì••ì¶• ê°€ëŠ¥í•œ ê¸°ì–µ ì°¾ê¸°
        for collection_type in CollectionType:
            try:
                stats = await self.vector_store.get_collection_stats(collection_type)
                count = stats.get('document_count', stats.get('count', 0))
                logger.debug(f"ì»¬ë ‰ì…˜ {collection_type.value}: {count}ê°œ ë¬¸ì„œ")
                
                # TODO: ì‹¤ì œ ë‚ ì§œ ê¸°ë°˜ ì¿¼ë¦¬ êµ¬í˜„ ì‹œ ì¶”ê°€
                # í˜„ì¬ëŠ” ì»¬ë ‰ì…˜ í†µê³„ë§Œ í™•ì¸
                
            except Exception as e:
                logger.debug(f"ì»¬ë ‰ì…˜ {collection_type.value} ì••ì¶• ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ê¸°ì–µ ì••ì¶• ì™„ë£Œ: {compressed_count}ê°œ")
        return compressed_count
    
    async def archive_old_memories(self) -> int:
        """
        ì˜¤ë˜ëœ ê¸°ì–µë“¤ì„ ì•„ì¹´ì´ë¸Œ
        
        Returns:
            ì•„ì¹´ì´ë¸Œëœ ê¸°ì–µì˜ ìˆ˜
        """
        archived_count = 0
        
        # ëª¨ë“  ì»¬ë ‰ì…˜ í™•ì¸
        for collection_type in CollectionType:
            try:
                stats = await self.vector_store.get_collection_stats(collection_type)
                # ì„ì‹œë¡œ 10%ë¥¼ ì•„ì¹´ì´ë¸Œí–ˆë‹¤ê³  ê°€ì •
                count = stats.get('document_count', stats.get('count', 0))
                temp_archived = count // 10
                archived_count += temp_archived
                
            except Exception as e:
                logger.debug(f"ì»¬ë ‰ì…˜ {collection_type.value} ì•„ì¹´ì´ë¸Œ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ê¸°ì–µ ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: {archived_count}ê°œ")
        return archived_count
    
    async def update_statistics(self) -> MemoryStatistics:
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        now = datetime.now()
        
        # ìºì‹œ í™•ì¸ (5ë¶„ë§ˆë‹¤ ì—…ë°ì´íŠ¸)
        if (self._cached_statistics and 
            (now - self._last_statistics_update).seconds < 300):
            return self._cached_statistics
        
        stats = MemoryStatistics()
        
        # ëª¨ë“  ì»¬ë ‰ì…˜ì—ì„œ í†µê³„ ìˆ˜ì§‘
        for collection_type in CollectionType:
            try:
                collection_stats = await self.vector_store.get_collection_stats(collection_type)
                # document_count ë˜ëŠ” count í‚¤ í™•ì¸
                count = collection_stats.get('document_count', collection_stats.get('count', 0))
                stats.total_memories += count
                stats.active_memories += count  # ëª¨ë“  ë©”ëª¨ë¦¬ë¥¼ í™œì„±ìœ¼ë¡œ ê°„ì£¼
                logger.debug(f"ì»¬ë ‰ì…˜ {collection_type.value}: {count}ê°œ ë¬¸ì„œ")
            except Exception as e:
                logger.debug(f"ì»¬ë ‰ì…˜ {collection_type.value} í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        stats.archived_memories = len(self.archive)
        stats.average_importance = 0.5  # ê¸°ë³¸ê°’
        
        # ìºì‹œ ì—…ë°ì´íŠ¸
        self._cached_statistics = stats
        self._last_statistics_update = now
        
        return stats
    
    async def get_statistics(self) -> MemoryStatistics:
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        if self._cached_statistics is None:
            return await self.update_statistics()
        return self._cached_statistics
    
    async def _calculate_importance(self, memory: BaseMemory) -> float:
        """
        ê¸°ì–µì˜ ì¤‘ìš”ë„ë¥¼ ìë™ìœ¼ë¡œ ê³„ì‚°
        
        Args:
            memory: ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•  ê¸°ì–µ
            
        Returns:
            ì¤‘ìš”ë„ ì ìˆ˜ (0.0-1.0)
        """
        score = 0.0
        
        # 1. ì ‘ê·¼ ë¹ˆë„ (0.0-0.3)
        if memory.accessed_count > 0:
            access_score = min(0.3, math.log(1 + memory.accessed_count) * 0.1)
            score += access_score
        
        # 2. ìµœê·¼ì„± (0.0-0.3)
        days_since_access = (datetime.now() - memory.last_accessed).days
        if days_since_access < 7:
            recency_score = 0.3 * (1 - days_since_access / 7)
            score += recency_score
        
        # 3. ë‚´ìš© ë³µì¡ë„ (0.0-0.2)
        content_length = len(memory.content)
        if content_length > 100:
            complexity_score = min(0.2, content_length / 1000)
            score += complexity_score
        
        # 4. ê¸°ë³¸ ì¤‘ìš”ë„ (0.0-0.2)
        importance_map = {
            ImportanceLevel.CRITICAL: 0.2,
            ImportanceLevel.HIGH: 0.15,
            ImportanceLevel.MEDIUM: 0.1,
            ImportanceLevel.LOW: 0.05,
            ImportanceLevel.TRIVIAL: 0.0
        }
        score += importance_map.get(memory.importance, 0.1)
        
        return min(1.0, score)
    
    def _score_to_level(self, score: float) -> ImportanceLevel:
        """ì ìˆ˜ë¥¼ ì¤‘ìš”ë„ ë ˆë²¨ë¡œ ë³€í™˜"""
        if score >= 0.8:
            return ImportanceLevel.CRITICAL
        elif score >= 0.6:
            return ImportanceLevel.HIGH
        elif score >= 0.4:
            return ImportanceLevel.MEDIUM
        elif score >= 0.2:
            return ImportanceLevel.LOW
        else:
            return ImportanceLevel.TRIVIAL
    
    async def _check_storage_limits(self) -> None:
        """ìŠ¤í† ë¦¬ì§€ í•œê³„ ì²´í¬ ë° ìë™ ì •ë¦¬"""
        stats = await self.get_statistics()
        
        if stats.active_memories > self.max_active_memories:
            # ì˜¤ë˜ëœ ê¸°ì–µë“¤ ìë™ ì•„ì¹´ì´ë¸Œ
            excess = stats.active_memories - self.max_active_memories
            logger.info(f"ìŠ¤í† ë¦¬ì§€ í•œê³„ ì´ˆê³¼, {excess}ê°œ ê¸°ì–µ ì •ë¦¬ ì‹œì‘")
            
            archived = await self.archive_old_memories()
            logger.info(f"ìë™ ì •ë¦¬ ì™„ë£Œ: {archived}ê°œ ì•„ì¹´ì´ë¸Œ")
    
    async def cleanup_old_memories(self) -> Tuple[int, int]:
        """
        ì˜¤ë˜ëœ ê¸°ì–µë“¤ ì •ë¦¬
        
        Returns:
            (ì••ì¶•ëœ ìˆ˜, ì•„ì¹´ì´ë¸Œëœ ìˆ˜)
        """
        logger.info("ê¸°ì–µ ì •ë¦¬ ì‘ì—… ì‹œì‘")
        
        compressed = await self.compress_old_memories()
        archived = await self.archive_old_memories()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        await self.update_statistics()
        
        logger.info(f"ê¸°ì–µ ì •ë¦¬ ì™„ë£Œ: ì••ì¶• {compressed}ê°œ, ì•„ì¹´ì´ë¸Œ {archived}ê°œ")
        return compressed, archived


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
async def create_memory_manager(data_path: str = "data/memory",
                              llm_provider: Optional[LLMProvider] = None) -> MemoryManager:
    """MemoryManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì´ˆê¸°í™”"""
    from .embedding_provider import get_embedding_provider
    
    # ì„ë² ë”© í”„ë¡œë°”ì´ë” ë¹„ë™ê¸° ìƒì„±
    embedding_provider = await get_embedding_provider()
    
    # VectorStore ìƒì„±
    vector_store = VectorStore(
        data_path=data_path,
        llm_provider=llm_provider,
        embedding_provider=embedding_provider
    )
    
    # RAGEngine ìƒì„± (ì„ íƒì )
    rag_engine = None
    if llm_provider:
        try:
            rag_engine = RAGSearchEngine(
                vector_store=vector_store,
                embedding_provider=embedding_provider
            )
        except Exception as e:
            logger.warning(f"RAG ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}")
    
    # MemoryManager ìƒì„± ë° ì´ˆê¸°í™”
    memory_manager = MemoryManager(
        vector_store=vector_store,
        rag_engine=rag_engine,
        llm_provider=llm_provider
    )
    
    await memory_manager.initialize()
    return memory_manager


def format_statistics(stats: MemoryStatistics) -> str:
    """í†µê³„ ì •ë³´ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…"""
    return f"""
ğŸ“Š ê¸°ì–µ ì‹œìŠ¤í…œ í†µê³„

ì „ì²´ í˜„í™©:
  â€¢ ì´ ê¸°ì–µ ìˆ˜: {stats.total_memories:,}ê°œ
  â€¢ í™œì„± ê¸°ì–µ: {stats.active_memories:,}ê°œ
  â€¢ ì•„ì¹´ì´ë¸Œ: {stats.archived_memories:,}ê°œ
  â€¢ í‰ê·  ì¤‘ìš”ë„: {stats.average_importance:.3f}

ì €ì¥ í˜„í™©:
  â€¢ ì‚¬ìš©ëŸ‰: {stats.storage_usage_mb:.2f} MB
  â€¢ ê°€ì¥ ì˜¤ë˜ëœ ê¸°ì–µ: {stats.oldest_memory_age_days}ì¼ ì „
  â€¢ ê°€ì¥ ìµœê·¼ ê¸°ì–µ: {stats.newest_memory_age_days}ì¼ ì „

íƒ€ì…ë³„ ë¶„í¬:
{chr(10).join(f'  â€¢ {memory_type.value}: {count}ê°œ' for memory_type, count in stats.memory_by_type.items())}

ì¤‘ìš”ë„ë³„ ë¶„í¬:
{chr(10).join(f'  â€¢ {level.value}: {count}ê°œ' for level, count in stats.memory_by_importance.items())}
"""
