"""Discord Bot AI ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ (ì—ì´ì „í‹± AI ì—”ì§„ í†µí•©)

Discord Botê³¼ ì—ì´ì „í‹± AI ì—”ì§„ ê°„ì˜ ë©”ì‹œì§€ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆ
ê°œë°œ ê³„íšì„œ Phase 3.3: ì§„ì •í•œ AI ì—ì´ì „íŠ¸ êµ¬í˜„
"""

import asyncio
from datetime import datetime
from dataclasses import dataclass
from typing import Optional, List, Dict, Any
from loguru import logger

# AI ì—”ì§„ ê´€ë ¨ import
from ..ai_engine.llm_provider import GeminiProvider, ChatMessage
from ..ai_engine.decision_engine import AgenticDecisionEngine, ActionType, DecisionResult
from ..mcp.registry import ToolRegistry  
from ..config import Settings


@dataclass
class AIResponse:
    """AI ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    confidence: float = 1.0
    reasoning: str = "AI processing"
    needs_followup: bool = False
    tool_calls_made: List[str] = None
    metadata: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.tool_calls_made is None:
            self.tool_calls_made = []

import asyncio
from typing import Optional, Dict, Any
from dataclasses import dataclass
from datetime import datetime

from src.utils.logger import get_logger
from src.ai_engine.llm_provider import GeminiProvider
from src.config import Settings

logger = get_logger(__name__)

@dataclass
class AIResponse:
    """AI ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    confidence: float
    reasoning: Optional[str] = None
    suggested_actions: Optional[list] = None
    needs_followup: bool = False
    metadata: Optional[Dict[str, Any]] = None

class AIMessageHandler:
    """AI ë©”ì‹œì§€ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.llm_provider = None
        self._initialize_ai_engine()
        
    def _initialize_ai_engine(self):
        """AI ì—”ì§„ ì´ˆê¸°í™”"""
        try:
            if self.settings.has_valid_ai_api_key():
                self.llm_provider = GeminiProvider(self.settings)
                # GeminiProvider ì´ˆê¸°í™” í˜¸ì¶œ
                asyncio.create_task(self._async_initialize_gemini())
                logger.info("AI ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. Mock ëª¨ë“œë¡œ ë™ì‘")
                self.llm_provider = None
        except Exception as e:
            logger.error(f"AI ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm_provider = None
    
    async def _async_initialize_gemini(self):
        """ë¹„ë™ê¸° Gemini ì´ˆê¸°í™”"""
        try:
            if self.llm_provider:
                success = await self.llm_provider.initialize()
                if success:
                    logger.info("Gemini Provider ë¹„ë™ê¸° ì´ˆê¸°í™” ì„±ê³µ")
                else:
                    logger.error("Gemini Provider ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨")
        except Exception as e:
            logger.error(f"Gemini Provider ë¹„ë™ê¸° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def process_message(self, user_message: str, user_id: str, channel_id: str) -> AIResponse:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ AI ì—”ì§„ìœ¼ë¡œ ì²˜ë¦¬"""
        if not self.llm_provider:
            return AIResponse(
                content="AI ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.",
                confidence=0.0,
                metadata={"error": "AI ì—”ì§„ ë¯¸ì´ˆê¸°í™”"}
            )
        
        # Geminiê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë‹¤ì‹œ ì‹œë„
        if not self.llm_provider.is_available():
            logger.info("Gemini Provider ì¬ì´ˆê¸°í™” ì‹œë„")
            try:
                success = await self.llm_provider.initialize()
                if not success:
                    logger.error("Gemini Provider ì¬ì´ˆê¸°í™” ì‹¤íŒ¨")
                    return AIResponse(
                        content="AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        confidence=0.0,
                        metadata={"error": "Gemini ì´ˆê¸°í™” ì‹¤íŒ¨"}
                    )
            except Exception as e:
                logger.error(f"Gemini Provider ì¬ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
                return AIResponse(
                    content="AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    confidence=0.0,
                    metadata={"error": str(e)}
                )
        
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            messages = self._build_prompt(user_message, user_id)
            
            # AI ì‘ë‹µ ìƒì„±
            response = await self.llm_provider.generate_response(messages)
            
            # ì‘ë‹µ íŒŒì‹±
            ai_response = self._parse_ai_response(response.content, user_message)
            
            logger.info(f"AI ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ: {user_id} -> {ai_response.content[:50]}...")
            return ai_response
            
        except Exception as e:
            logger.error(f"AI ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return AIResponse(
                content="ì£„ì†¡í•©ë‹ˆë‹¤. ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                confidence=0.0,
                metadata={"error": str(e)}
            )
    
    def _build_prompt(
        self, 
        user_message: str, 
        user_id: str
    ) -> List[ChatMessage]:
        """AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        system_prompt = """ë‹¹ì‹ ì€ ê°œì¸ AI ë¹„ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ë„ë¡ ì‘ë‹µí•´ì£¼ì„¸ìš”.

ì‘ë‹µ ì§€ì¹¨:
1. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µ
2. ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
3. í•„ìš”ì‹œ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ í™•ì¸ ìš”ì²­
4. í•  ìˆ˜ ì—†ëŠ” ì¼ì€ ì†”ì§í•˜ê²Œ ì„¤ëª…
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”"""

        messages = [
            ChatMessage(role="system", content=system_prompt),
            ChatMessage(role="user", content=user_message)
        ]
        
        return messages
        
        return base_prompt
    
    def _parse_ai_response(self, ai_response: str, original_message: str) -> AIResponse:
        """AI ì‘ë‹µ íŒŒì‹±"""
        try:
            # ê¸°ë³¸ ì‘ë‹µ êµ¬ì„±
            response = AIResponse(
                content=ai_response.strip(),
                confidence=0.9,  # ê¸°ë³¸ ì‹ ë¢°ë„
                reasoning="AI ìì—°ì–´ ì²˜ë¦¬",
                metadata={
                    "original_message": original_message,
                    "processed_at": datetime.now().isoformat(),
                    "model": "gemini-2.5-pro"
                }
            )
            
            # ì‘ë‹µ ê¸¸ì´ì— ë”°ë¥¸ ì‹ ë¢°ë„ ì¡°ì •
            if len(ai_response.strip()) < 10:
                response.confidence = 0.6
            elif len(ai_response.strip()) > 1000:
                response.confidence = 0.95
            
            # íŠ¹ì • íŒ¨í„´ì— ë”°ë¥¸ í›„ì† ì§ˆë¬¸ í•„ìš”ì„± íŒë‹¨
            followup_keywords = ["ë” ì•Œë ¤ì£¼ì„¸ìš”", "êµ¬ì²´ì ìœ¼ë¡œ", "ìì„¸íˆ", "ì¶”ê°€ë¡œ", "?"]
            if any(keyword in ai_response for keyword in followup_keywords):
                response.needs_followup = True
            
            return response
            
        except Exception as e:
            logger.error(f"AI ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return AIResponse(
                content=ai_response.strip(),
                confidence=0.5,
                reasoning="íŒŒì‹± ì˜¤ë¥˜",
                metadata={"error": str(e)}
            )
    
    async def _mock_ai_response(self, user_message: str, user_name: str) -> AIResponse:
        """Mock AI ì‘ë‹µ (API í‚¤ê°€ ì—†ì„ ë•Œ)"""
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ ì‘ë‹µ
        message_lower = user_message.lower()
        
        if any(greeting in message_lower for greeting in ["ì•ˆë…•", "hello", "hi", "í—¬ë¡œ"]):
            content = f"ì•ˆë…•í•˜ì„¸ìš”, {user_name}ë‹˜! ğŸ‘‹ ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        elif any(keyword in message_lower for keyword in ["ë‚ ì”¨", "weather"]):
            content = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤."
        elif any(keyword in message_lower for keyword in ["ë„ì›€", "help", "í—¬í”„"]):
            content = """ğŸ¤– AI ë¹„ì„œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ì¼ë“¤:

â€¢ ì¼ë°˜ì ì¸ ì§ˆë¬¸ ë‹µë³€
â€¢ ì¼ì • ê´€ë¦¬ (Notion ì—°ë™ ì‹œ)
â€¢ ê°„ë‹¨í•œ ê³„ì‚° ë° ì •ë³´ ê²€ìƒ‰
â€¢ Apple ì•± ì—°ë™ (ì„¤ì • ì‹œ)

ë” ìì„¸í•œ ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” AI API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤."""
        elif "í…ŒìŠ¤íŠ¸" in message_lower or "test" in message_lower:
            content = f"âœ… AI ë¹„ì„œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!\n\nğŸ“± Discord ì—°ê²°: ì •ìƒ\nğŸ¤– ë©”ì‹œì§€ ì²˜ë¦¬: ì •ìƒ\nğŸ‘¤ ì‚¬ìš©ì: {user_name}\nâ° ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        else:
            content = f"ë©”ì‹œì§€ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤: \"{user_message}\"\n\ní˜„ì¬ëŠ” ì œí•œëœ ê¸°ëŠ¥ìœ¼ë¡œ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤. ì™„ì „í•œ AI ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” Gemini API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        await asyncio.sleep(0.5)  # ì‹¤ì œ AI ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        
        return AIResponse(
            content=content,
            confidence=0.8,
            reasoning="Mock AI ì‘ë‹µ",
            metadata={
                "mode": "mock",
                "original_message": user_message,
                "processed_at": datetime.now().isoformat()
            }
        )
    
    def is_available(self) -> bool:
        """AI ì—”ì§„ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self.llm_provider is not None
    
    async def get_status(self) -> Dict[str, Any]:
        """AI í•¸ë“¤ëŸ¬ ìƒíƒœ ì •ë³´"""
        status = {
            "ai_engine_available": self.is_available(),
            "api_key_configured": self.settings.has_valid_ai_api_key(),
            "model": "gemini-2.5-pro" if self.is_available() else "mock",
            "mode": "production" if self.is_available() else "mock"
        }
        
        if self.llm_provider:
            try:
                # LLM ì œê³µì ìƒíƒœ í™•ì¸
                provider_available = self.llm_provider.is_available()
                status["llm_provider_available"] = provider_available
            except Exception as e:
                status["provider_error"] = str(e)
        
        return status

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_ai_handler: Optional[AIMessageHandler] = None

def get_ai_handler() -> AIMessageHandler:
    """ì „ì—­ AI í•¸ë“¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _ai_handler
    if _ai_handler is None:
        logger.info("AI Handler ìƒˆë¡œ ìƒì„±")
        settings = Settings()
        _ai_handler = AIMessageHandler(settings)
        logger.info("AI Handler ì´ˆê¸°í™” ì™„ë£Œ")
    return _ai_handler

async def process_discord_message(
    user_message: str,
    user_id: int, 
    user_name: str,
    context: Optional[Dict[str, Any]] = None
) -> str:
    """
    Discord ë©”ì‹œì§€ë¥¼ AIë¡œ ì²˜ë¦¬í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Returns:
        AI ì‘ë‹µ ë¬¸ìì—´
    """
    handler = get_ai_handler()
    response = await handler.process_message(user_message, str(user_id), "general")
    return response.content
